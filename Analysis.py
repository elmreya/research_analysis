# -*- coding: utf-8 -*-
"""Final Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cDL5WAwXFT4Rq0yHFmm58H62kKCkzsJE
"""

import numpy as np
import pymc as pm
import matplotlib.pyplot as plt
import arviz as az
import pandas as pd
import ipywidgets as widgets
from scipy import stats
from graphviz import Digraph
import statistics

"""Methodology and Main Results:

The paper designed an experiment to measure the effect of 'power posing' on job interviews. The experiment included 61 participants that were randomly assigned to either a 'low power' or 'high power' condition. Participants in the low power condition were asked to stand with their feet together and crossed over, and their hands wrapped around their torso, with the idea to take up as little space as possible. In contrast, participants in the 'high power' condition were instructed to stand with their two feet apart, and take up more space. One minute into holding their poses, the participants were asked to imagine they were to interview for their dream job and prepare a 5-minute interview speech, talking about their qualifications and why they should be hired for the job. They were told that they'd have to deliver this speech to two experienced evaluators. These two evaluators didn't evaluate the dependent variables, however they did interact with the participants and were aware of their conditions, so there is a high potential for experimenter bias in this study.
After around 5-6 minutes of the participants holding their poses and preparing their speeches, the evaluators asked them to stop holding their pose and give their interview speech.
Their speeches were recorded and then evaluated for overall performance and hireability. Their performances were evaluated by two condition and hypothesis blind 'coders'. These coders were undergraduate coders who had 'experience in coding verbal and non-verbal content'. However, they did not have experience in interviewing candidates.

The authors of this paper hypothesized that the group standing in the high power poses would perform better on the job interview speech than the group that stood in low power pose.

Their results showed that the participants that maintained the high power pose scored higher in both overall performance and hireability. They also theorized about a plausible mediator, but for this analysis, I will be using the total effect presented in the results.

In their results, the dependent variable 'Overall performance', has an average of 4.62 and a standard deviation of 1.16 for the participants in the 'high power' condition, and an average of 3.81 with a standard deviation of
1.08 for the participants in the 'low power' condition.
For the other dependent variable 'hireability', the avg for the participants in the high power condition was 2.43 with a standard deviation of 0.63 and the avg for the participants in the low power condition was 2.00 with a standard deviation of 0.63.


In my analysis, I only use the overall performance dependent variable.






"""

# Model 1: In favour

sample_size = 61

primed = np.random.randint(low = 0, high = 2, size = sample_size)

noise = stats.norm(0,1).rvs(sample_size)

overall_performance = 3.81 + 0.82*primed + noise

statistics.stdev(overall_performance)

with pm.Model() as m1:

    a = pm.Normal('Intercept',4,1)
    b1 = pm.Normal('Priming',0,1)
    s = pm.Exponential('Noise',1)

    mean = a + b1*primed

    y = pm.Normal('overall_performance',mean,s,observed=overall_performance)

    trace1 = pm.sample()

az.plot_posterior(trace1);

"""The model in favor simulates data with the same effect size and variation as the paper, and upon several trials of simulations (where I simulated both the simulation, and the pymc model many times) pymc is somewhat able to estimate the effect size of priming, at least in the ballpark of the actual value, however there is a lot of variation still, and pymc's guesses are sometimes way off.

However, because on average, it can guess the values somewhat around the 0.82 effect size, we can say that the sample size was probably sufficiently large if the true effect size of dependent variable (overall performance) was what they found in their paper.

Alternative explanation:

There are many reasons to believe that the effect size of power posing might be smaller than reported. Certain aspects of the paper, such as a strong potential for experimenter bias.
As well as the 'coders' that evaluate the participants' interview speeches were undegraduate students with no experience in evaluating job interviews. Considering all of this, it is reasonable to believe that their reported effect size might be too high. In order to create a second model that takes into account these aspects of the experiment, I will use a smaller effect size, and furthermore, something like an experimenter bias can incorrectly cause a smaller variation in the dependent variables, and therefore, in order to take this into account, for this 'bad case scenario' I will incorporate more variability.

Given this model, the sample size (61 participants) might not be large enough to capture the effect reliably.
"""

# Model 2: Alternative Explanation

sample_size = 61

primed_a = np.random.randint(low = 0, high = 2, size = sample_size)

noise_a = stats.norm(0,2).rvs(sample_size)

overall_performance_a = 3.81 + 0.6*primed_a + noise_a

with pm.Model() as alt:
    a = pm.Normal('Intercept',4,1)
    b1 = pm.Normal('Priming',0,1)
    s = pm.Exponential('Noise',1)

    mean = a + b1*primed_a

    y = pm.Normal('overall_performance',mean,s,observed=overall_performance_a)

    trace = pm.sample()

az.plot_posterior(trace);

"""The alternative explanation model simulates data that has more variation and has a smaller effect size. Using this data, from multile trials (where I simulated both the data and the pymc model multiple times), pymc's guesses were usually further away from the effect size of 0.6.

Both the model in favour and the alternative explanation model vary, and sometimes get their values around the correct value, but the alternative explanation model's pymc guesses are further away, and more inaccurate than in the 'good case scenario'.
Therefore, even if we assume that the effect size they reported was large enough given the sample size, the more reasonable true smaller effect size is not large enough to be reliably detected given the sample size.

The more plausible model:
The alternative model is more plausible as there are many aspects of the study (experiment bias, etc) that makes is very reasonable to assume a smaller effect size. Given this smaller effect size, pymc model is more inaccurate in its guesses, and it is sufficiently inaccurate to justify concluding that the sample size that was used in the paper was not large enough.
"""